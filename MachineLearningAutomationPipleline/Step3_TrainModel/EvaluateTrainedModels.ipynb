{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118e46f2",
   "metadata": {},
   "source": [
    "## Convert this notebook to executable python script using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a8cc0",
   "metadata": {},
   "source": [
    "- jupyter nbconvert --to python EvaluateTrainedModels.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd880a5b",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import json\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from datetime import date, datetime, timedelta, time\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainModel_Helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a715c75",
   "metadata": {},
   "source": [
    "# Read the Input JSON File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beaf988",
   "metadata": {},
   "source": [
    "### Input file name when using jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df768d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_eval_models = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/InputJson/Eval/json_eval_000.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd32a84",
   "metadata": {},
   "source": [
    "### Input file name when using python script on command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ef775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_file_eval_models = sys.argv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f8011",
   "metadata": {},
   "source": [
    "### Load the JSON file for evaluating trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for evaluating trained models: \\n {}'.format(json_file_eval_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_eval_models) as json_file_handle:\n",
    "    json_content_eval_models = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_eval_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d22b6",
   "metadata": {},
   "source": [
    "# Simulation Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dir = json_content_eval_models['paths']['sim_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1cc6a",
   "metadata": {},
   "source": [
    "# `json` Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ed949",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_extract_base = json_content_eval_models['paths']['json_extract_base']\n",
    "json_prep_base = json_content_eval_models['paths']['json_prep_base']\n",
    "json_train_base = json_content_eval_models['paths']['json_train_base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_extract_base = os.path.join(sim_dir, json_extract_base)\n",
    "json_prep_base = os.path.join(sim_dir, json_prep_base)\n",
    "json_train_base = os.path.join(sim_dir, json_train_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1a119",
   "metadata": {},
   "source": [
    "# Collect Metrics of Desired Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_options = json_content_eval_models['collection_options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_extract_counts = collection_options['json_extract_counts']\n",
    "json_prep_train_maps = collection_options['json_prep_train_maps']\n",
    "FM_label_type = collection_options['FM_label_type']\n",
    "metric_names = collection_options['metric_names']\n",
    "metric_on_sets = collection_options['metric_on_sets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfe46f",
   "metadata": {},
   "source": [
    "## Create label and train pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98232959",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_pair, col_names = create_label_train_pair (json_prep_train_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_extract_counts\n",
    "#label_train_pair\n",
    "#col_names\n",
    "#metric_names\n",
    "#metric_on_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7061fd",
   "metadata": {},
   "source": [
    "## Create data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_defn = create_data_definition (json_extract_base, json_extract_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_defn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35675f",
   "metadata": {},
   "source": [
    "## Collect evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = metric_names[0]\n",
    "metric_on_set = metric_on_sets[3]\n",
    "eval_metric_col = '{}_{}'.format(metric_name, metric_on_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a66650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_metric_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = gather_metrics_for_all_label_train_pairs (label_train_pair, col_names, \\\n",
    "                                                  json_train_base, json_extract_counts, \\\n",
    "                                                  FM_label_type, eval_metric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2be330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbc35c",
   "metadata": {},
   "source": [
    "## Plot the gathered evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3385a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name in metric_names:\n",
    "    for metric_on_set in metric_on_sets:\n",
    "        ylabel_text, title_text = get_labels_title_for_plots (FM_label_type, \\\n",
    "                                                              metric_name, metric_on_set)\n",
    "        #print('ylabel_text: {}, title_text: {}'.format(ylabel_text, title_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "trained_models_metrics, data_defn = create_trained_models_metrics (\\\n",
    "                                      json_prep_base, json_prep_counts, \\\n",
    "                                      json_train_base, json_train_counts, \\\n",
    "                                      json_extract_base, json_extract_counts)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38894325",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_train_combined = pd.DataFrame()\n",
    "df_test_combined = pd.DataFrame()\n",
    "for metric_name in metric_names:\n",
    "    df_train, df_test = plot_trained_models_metrics (FM_label_type, json_extract_counts, \\\n",
    "                                                     trained_models_metrics, metric_name)\n",
    "    for col in df_train.columns:\n",
    "        df_train = df_train.rename(columns = {col: col + '-' + metric_name})\n",
    "        df_train_combined = pd.concat([df_train_combined, df_train], axis=1)  \n",
    "        \n",
    "        df_test = df_test.rename(columns = {col: col + '-' + metric_name})\n",
    "        df_test_combined = pd.concat([df_test_combined, df_test], axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84439442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_defn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af77cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e4f25",
   "metadata": {},
   "source": [
    "## Effect of Max History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc67883",
   "metadata": {},
   "source": [
    "## Effect of History Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061ca89",
   "metadata": {},
   "source": [
    "## Effect of Temporal Data Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99752a80",
   "metadata": {},
   "source": [
    "## Effect of Spatial Data Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a1427",
   "metadata": {},
   "source": [
    "## Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "json_extract_counts = [15, 16, 17] #[2, 3, 4, 5, 6, 7, 8]\n",
    "json_prep_counts = [1]\n",
    "json_train_counts = [1, 3]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc10a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_data_size_scatter = 800\n",
    "fig_size_x = 8\n",
    "fig_size_y = 8\n",
    "font_size  = 10\n",
    "x_lim      = [0, 0.7]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95508b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_identifier = \"Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0efc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "label_count = 1 # Regression\n",
    "json_prep    = '%s_%03d.json'%(json_prep_base, label_count)\n",
    "#print(json_prep)\n",
    "with open(json_prep) as json_file_handle:\n",
    "    json_content_prep_data = json.load(json_file_handle)\n",
    "label_count = json_content_prep_data['label_defn']['label_count']\n",
    "FM_label_type = json_content_prep_data['FM_labels']['label_type']\n",
    "#print('label_count: {}, FM_label_type: {}\\n'.format(label_count, FM_label_type))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(len(json_extract_counts), len(json_train_counts), figsize=(12, 8))\n",
    "\n",
    "for train_count_ind, train_count in enumerate(json_train_counts):\n",
    "    json_train   = '%s_%03d.json'%(json_train_base, train_count)\n",
    "    #print(json_train)\n",
    "    with open(json_train) as json_file_handle:\n",
    "        json_content_train_model = json.load(json_file_handle)\n",
    "    model_count = json_content_train_model['models']['model_count']\n",
    "    model_name = json_content_train_model['models']['model_name'] # ['RF', SVM', 'MLP']\n",
    "    #print('Model Count: {}, Model Name: {}\\n'.format(model_count, model_name))\n",
    "\n",
    "    for data_count_ind, data_count in enumerate(json_extract_counts):\n",
    "        json_extract = '%s_%03d.json'%(json_extract_base, data_count)\n",
    "        #print(json_extract)\n",
    "        with open(json_extract) as json_file_handle:\n",
    "            json_content_extract_data = json.load(json_file_handle)\n",
    "        data_set_count = json_content_extract_data['data_set_defn']['data_set_count']\n",
    "        #print('Data Set Count: {}'.format(data_set_count))\n",
    "\n",
    "        # Names of trained model and related files\n",
    "        trained_model_base_loc = json_content_train_model['paths']['trained_model_base_loc']\n",
    "        trained_model_name = 'dataset_%03d_label_%03d_%s_model_%03d_%s'%(data_set_count, \\\n",
    "                                                    label_count, FM_label_type, \\\n",
    "                                                    model_count, model_name)\n",
    "\n",
    "        trained_model_loc = os.path.join(trained_model_base_loc, trained_model_name)\n",
    "        trained_model_file_name = '{}_model.pkl'.format(trained_model_name)\n",
    "        \n",
    "        train_data_features_file_name  = '{}_features_train.pkl'.format(trained_model_name)\n",
    "        train_data_labels_file_name    = '{}_labels_train.pkl'.format(trained_model_name)\n",
    "\n",
    "        test_data_features_file_name   = '{}_features_test.pkl'.format(trained_model_name)\n",
    "        test_data_labels_file_name     = '{}_labels_test.pkl'.format(trained_model_name)\n",
    "        \n",
    "        print('trained_model_file_name: {}'.format(trained_model_file_name))\n",
    "        #print('train_data_features_file_name: {}'.format(train_data_features_file_name))\n",
    "        #print('train_data_labels_file_name: {}'.format(train_data_labels_file_name))\n",
    "        #print('test_data_features_file_name: {}'.format(test_data_features_file_name))\n",
    "        #print('test_data_labels_file_name: {}'.format(test_data_labels_file_name))\n",
    "        \n",
    "        trained_model_file = os.path.join(trained_model_loc, trained_model_file_name)\n",
    "        model = pickle.load(open(trained_model_file, 'rb'))\n",
    "        #print ('\\nLoaded the ML model file at: {}\\n'.format(trained_model_file))\n",
    "        #print ('The model loaded is: {} \\n'.format(model))\n",
    "        \n",
    "        if (data_identifier == \"Train\"):\n",
    "            #print('Loading the saved features and labels used in training')\n",
    "            features_train = pickle.load(open(os.path.join(\\\n",
    "                                    trained_model_loc, train_data_features_file_name), 'rb'))\n",
    "            labels_train   =  pickle.load(open(os.path.join(\\\n",
    "                                    trained_model_loc, train_data_labels_file_name), 'rb'))\n",
    "            labels_pred_train = predict(model, features_train, \"Train Data\")\n",
    "            accuracy_train = get_accuracy_score(model, FM_label_type, \\\n",
    "                                   features_train, labels_train, labels_pred_train,\\\n",
    "                                   \"Train Data\")\n",
    "            labels_gt = labels_train\n",
    "            labels_pred = labels_pred_train\n",
    "            accuracy = accuracy_train\n",
    "        else:\n",
    "            #print('Loading the saved features and labels meant for testing')\n",
    "            features_test = pickle.load(open(os.path.join(\\\n",
    "                            trained_model_loc, test_data_features_file_name), 'rb'))\n",
    "            labels_test   =  pickle.load(open(os.path.join(\\\n",
    "                                    trained_model_loc, test_data_labels_file_name), 'rb'))\n",
    "            labels_pred_test = predict(model, features_test, \"Test Data\")\n",
    "            accuracy_test = get_accuracy_score(model, FM_label_type, \\\n",
    "                                   features_test, labels_test, labels_pred_test,\\\n",
    "                                   \"Test Data\")\n",
    "            \n",
    "            labels_gt = labels_test\n",
    "            labels_pred = labels_pred_test\n",
    "            accuracy = accuracy_test\n",
    "        \n",
    "        labels_gt_range = [labels_gt.min(), labels_gt.max()]\n",
    "        data_indices = range(len(labels_gt))\n",
    "        if (max_data_size_scatter < 1):\n",
    "            data_ind_subset = data_indices\n",
    "        else:\n",
    "            data_ind_subset = random.sample(data_indices, k = max_data_size_scatter)\n",
    "            \n",
    "        ax[data_count_ind, train_count_ind].scatter(labels_gt[data_ind_subset], labels_pred[data_ind_subset])\n",
    "        ax[data_count_ind, train_count_ind].plot(labels_gt_range, labels_gt_range, '--r')\n",
    "        ax[data_count_ind, train_count_ind].set_xlabel('Ground Truth', fontsize = font_size)\n",
    "        ax[data_count_ind, train_count_ind].set_ylabel('Prediction', fontsize = font_size)\n",
    "        ax[data_count_ind, train_count_ind].set_title('Model: {}, Accuracy: {:.3f}'.format(model_name, accuracy), fontsize = font_size)\n",
    "        ax[data_count_ind, train_count_ind].set_xlim(x_lim)\n",
    "        ax[data_count_ind, train_count_ind].set_ylim(x_lim)\n",
    "        #ax[data_count_ind, train_count_ind].set_yticks(fontsize = font_size, rotation = 0)\n",
    "        #ax[data_count_ind, train_count_ind].set_xticks(fontsize = font_size, rotation = 0)\n",
    "        #print('\\n')\n",
    "\n",
    "#print('\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_identifier = \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c55970",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "label_count = 1 # Regression\n",
    "json_prep    = '%s_%03d.json'%(json_prep_base, label_count)\n",
    "#print(json_prep)\n",
    "with open(json_prep) as json_file_handle:\n",
    "    json_content_prep_data = json.load(json_file_handle)\n",
    "label_count = json_content_prep_data['label_defn']['label_count']\n",
    "FM_label_type = json_content_prep_data['FM_labels']['label_type']\n",
    "#print('label_count: {}, FM_label_type: {}\\n'.format(label_count, FM_label_type))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(len(json_extract_counts), len(json_train_counts), figsize=(12, 8))\n",
    "\n",
    "for train_count_ind, train_count in enumerate(json_train_counts):\n",
    "    json_train   = '%s_%03d.json'%(json_train_base, train_count)\n",
    "    #print(json_train)\n",
    "    with open(json_train) as json_file_handle:\n",
    "        json_content_train_model = json.load(json_file_handle)\n",
    "    model_count = json_content_train_model['models']['model_count']\n",
    "    model_name = json_content_train_model['models']['model_name'] # ['RF', SVM', 'MLP']\n",
    "    #print('Model Count: {}, Model Name: {}\\n'.format(model_count, model_name))\n",
    "\n",
    "    for data_count_ind, data_count in enumerate(json_extract_counts):\n",
    "        json_extract = '%s_%03d.json'%(json_extract_base, data_count)\n",
    "        #print(json_extract)\n",
    "        with open(json_extract) as json_file_handle:\n",
    "            json_content_extract_data = json.load(json_file_handle)\n",
    "        data_set_count = json_content_extract_data['data_set_defn']['data_set_count']\n",
    "        #print('Data Set Count: {}'.format(data_set_count))\n",
    "\n",
    "        # Names of trained model and related files\n",
    "        trained_model_base_loc = json_content_train_model['paths']['trained_model_base_loc']\n",
    "        trained_model_name = 'dataset_%03d_label_%03d_%s_model_%03d_%s'%(data_set_count, \\\n",
    "                                                    label_count, FM_label_type, \\\n",
    "                                                    model_count, model_name)\n",
    "\n",
    "        trained_model_loc = os.path.join(trained_model_base_loc, trained_model_name)\n",
    "        trained_model_file_name = '{}_model.pkl'.format(trained_model_name)\n",
    "        \n",
    "        train_data_features_file_name  = '{}_features_train.pkl'.format(trained_model_name)\n",
    "        train_data_labels_file_name    = '{}_labels_train.pkl'.format(trained_model_name)\n",
    "\n",
    "        test_data_features_file_name   = '{}_features_test.pkl'.format(trained_model_name)\n",
    "        test_data_labels_file_name     = '{}_labels_test.pkl'.format(trained_model_name)\n",
    "        \n",
    "        #print('trained_model_file_name: {}'.format(trained_model_file_name))\n",
    "        #print('train_data_features_file_name: {}'.format(train_data_features_file_name))\n",
    "        #print('train_data_labels_file_name: {}'.format(train_data_labels_file_name))\n",
    "        #print('test_data_features_file_name: {}'.format(test_data_features_file_name))\n",
    "        #print('test_data_labels_file_name: {}'.format(test_data_labels_file_name))\n",
    "        \n",
    "        trained_model_file = os.path.join(trained_model_loc, trained_model_file_name)\n",
    "        model = pickle.load(open(trained_model_file, 'rb'))\n",
    "        #print ('\\nLoaded the ML model file at: {}\\n'.format(trained_model_file))\n",
    "        #print ('The model loaded is: {} \\n'.format(model))\n",
    "        \n",
    "        if (data_identifier == \"Train\"):\n",
    "            #print('Loading the saved features and labels used in training')\n",
    "            features_train = pickle.load(open(os.path.join(\\\n",
    "                                    trained_model_loc, train_data_features_file_name), 'rb'))\n",
    "            labels_train   =  pickle.load(open(os.path.join(\\\n",
    "                                    trained_model_loc, train_data_labels_file_name), 'rb'))\n",
    "            labels_pred_train = predict(model, features_train, \"Train Data\")\n",
    "            accuracy_train = get_accuracy_score(model, FM_label_type, \\\n",
    "                                   features_train, labels_train, labels_pred_train,\\\n",
    "                                   \"Train Data\")\n",
    "            labels_gt = labels_train\n",
    "            labels_pred = labels_pred_train\n",
    "            accuracy = accuracy_train\n",
    "        else:\n",
    "            #print('Loading the saved features and labels meant for testing')\n",
    "            features_test = pickle.load(open(os.path.join(\\\n",
    "                            trained_model_loc, test_data_features_file_name), 'rb'))\n",
    "            labels_test   =  pickle.load(open(os.path.join(\\\n",
    "                                    trained_model_loc, test_data_labels_file_name), 'rb'))\n",
    "            labels_pred_test = predict(model, features_test, \"Test Data\")\n",
    "            accuracy_test = get_accuracy_score(model, FM_label_type, \\\n",
    "                                   features_test, labels_test, labels_pred_test,\\\n",
    "                                   \"Test Data\")\n",
    "            \n",
    "            labels_gt = labels_test\n",
    "            labels_pred = labels_pred_test\n",
    "            accuracy = accuracy_test\n",
    "        \n",
    "        labels_gt_range = [labels_gt.min(), labels_gt.max()]\n",
    "        data_indices = range(len(labels_gt))\n",
    "        if (max_data_size_scatter < 1):\n",
    "            data_ind_subset = data_indices\n",
    "        else:\n",
    "            data_ind_subset = random.sample(data_indices, k = max_data_size_scatter)\n",
    "            \n",
    "        ax[data_count_ind, train_count_ind].scatter(labels_gt[data_ind_subset], labels_pred[data_ind_subset])\n",
    "        ax[data_count_ind, train_count_ind].plot(labels_gt_range, labels_gt_range, '--r')\n",
    "        ax[data_count_ind, train_count_ind].set_xlabel('Ground Truth', fontsize = font_size)\n",
    "        ax[data_count_ind, train_count_ind].set_ylabel('Prediction', fontsize = font_size)\n",
    "        ax[data_count_ind, train_count_ind].set_title('Model: {}, Accuracy: {:.3f}'.format(model_name, accuracy), fontsize = font_size)\n",
    "        ax[data_count_ind, train_count_ind].set_xlim(x_lim)\n",
    "        ax[data_count_ind, train_count_ind].set_ylim(x_lim)\n",
    "        #ax[data_count_ind, train_count_ind].set_yticks(fontsize = font_size, rotation = 0)\n",
    "        #ax[data_count_ind, train_count_ind].set_xticks(fontsize = font_size, rotation = 0)\n",
    "        #print('\\n')\n",
    "\n",
    "#print('\\n')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_ml_conda",
   "language": "python",
   "name": "py3_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
