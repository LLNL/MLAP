{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert this notebook to executable python script using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jupyter nbconvert --to python TrainModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as path\n",
    "import psutil\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pickle\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from datetime import date, datetime, timedelta, time\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_running_file_dir = sys.path[0]\n",
    "current_running_file_par = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, os.path.join(current_running_file_par, 'Step1_ExtractData'))\n",
    "sys.path.insert(0, os.path.join(current_running_file_par, 'Step3_TrainModel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extract_DFM_Data_Helper import *\n",
    "from TrainModel_Helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Start Time and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = timer()\n",
    "process = psutil.Process(os.getpid())\n",
    "global_initial_memory = process.memory_info().rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Input JSON File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file name when using jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_extract_data = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/InputJson/Extract/json_extract_data_022.json'\n",
    "json_file_prep_data    = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/InputJson/Prep/json_prep_data_label_002.json'\n",
    "json_file_train_model  = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/InputJson/Train/json_train_model_003.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file name when using python script on command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_file_extract_data = sys.argv[1]\n",
    "#json_file_prep_data = sys.argv[2]\n",
    "#json_file_train_model = sys.argv[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoading the JSON file for extracting data: \\n {}'.format(json_file_extract_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_extract_data) as json_file_handle:\n",
    "    json_content_extract_data = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_extract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for preparing data: \\n {}'.format(json_file_prep_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_prep_data) as json_file_handle:\n",
    "    json_content_prep_data = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_prep_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for training model: \\n {}'.format(json_file_train_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_train_model) as json_file_handle:\n",
    "    json_content_train_model = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to be Used for Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current data set params\n",
    "data_set_count = json_content_extract_data['data_set_defn']['data_set_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Label, FM Threshold etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = json_content_prep_data['label_defn']['label_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_labels = json_content_prep_data['FM_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_label_type = FM_labels['label_type']\n",
    "\n",
    "if (FM_label_type == 'Binary'):\n",
    "    FM_binary_threshold = FM_labels['FM_binary_threshold']\n",
    "    class_labels = range(2)\n",
    "if (FM_label_type == 'MultiClass'):\n",
    "    FM_MC_levels = FM_labels['FM_MC_levels']\n",
    "    class_labels = range(len(FM_MC_levels) -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_hr = json_content_prep_data['qoi_to_plot']['FM_hr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Options, ML Model, and Params etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = json_content_train_model['train_options']['train_from_scratch']\n",
    "save_train_data = json_content_train_model['train_options']['save_train_data']\n",
    "save_test_data = json_content_train_model['train_options']['save_test_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count = json_content_train_model['models']['model_count']\n",
    "scaler_type = json_content_train_model['models']['scaler_type']\n",
    "model_name  = json_content_train_model['models']['model_name'] # ['RF', SVM', 'MLP']\n",
    "model_params = json_content_train_model['models']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features/Labels Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qois_for_training = json_content_train_model['features_labels']['qois_for_training']\n",
    "label_log = json_content_train_model['features_labels']['label_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qois_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = json_content_train_model['evaluation']\n",
    "fig_size_x = evaluation['fig_size_x']\n",
    "fig_size_y = evaluation['fig_size_y']\n",
    "font_size  = evaluation['font_size']\n",
    "\n",
    "if (FM_label_type == 'Regression'):\n",
    "    max_data_size_scatter = evaluation['max_data_size_scatter']\n",
    "    x_lim      = evaluation['x_lim']\n",
    "else:\n",
    "    normalize_cm = evaluation['normalize_cm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_base_loc = json_content_prep_data[ 'paths']['prepared_data_base_loc']\n",
    "trained_model_base_loc = json_content_train_model['paths']['trained_model_base_loc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet and Label Specific (Train and Test Data Prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_set_name = 'dataset_%03d_label_%03d_%s'%(data_set_count, \\\n",
    "                                                       label_count, FM_label_type)\n",
    "\n",
    "prepared_data_loc = os.path.join(prepared_data_base_loc, prepared_data_set_name)\n",
    "#os.system('mkdir -p %s'%prepared_data_loc)\n",
    "\n",
    "prepared_data_file_name = '{}.pkl'.format(prepared_data_set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet, Label, and Model Specific (Trained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = 'dataset_%03d_label_%03d_%s_model_%03d_%s'%(data_set_count, \\\n",
    "                                                        label_count, FM_label_type, \\\n",
    "                                                        model_count, model_name)\n",
    "\n",
    "trained_model_loc = os.path.join(trained_model_base_loc, trained_model_name)\n",
    "os.system('mkdir -p %s'%trained_model_loc)\n",
    "\n",
    "trained_model_file_name = '{}_model.pkl'.format(trained_model_name)\n",
    "\n",
    "train_data_features_file_name   = '{}_features_train.pkl'.format(trained_model_name)\n",
    "train_data_labels_file_name     = '{}_labels_train.pkl'.format(trained_model_name)\n",
    "\n",
    "train_data_scatter_file_name    = '{}_scatter_train.png'.format(trained_model_name)\n",
    "train_data_cm_file_name         = '{}_cm_train.png'.format(trained_model_name)\n",
    "\n",
    "test_data_features_file_name    = '{}_features_test.pkl'.format(trained_model_name)\n",
    "test_data_labels_file_name      = '{}_labels_test.pkl'.format(trained_model_name)\n",
    "\n",
    "test_data_scatter_file_name     = '{}_scatter_test.png'.format(trained_model_name)\n",
    "test_data_scatter_file_name_p90 = '{}_scatter_test_p90.png'.format(trained_model_name)\n",
    "test_data_scatter_file_name_p95 = '{}_scatter_test_p95.png'.format(trained_model_name)\n",
    "\n",
    "test_data_cm_file_name          = '{}_cm_test.png'.format(trained_model_name)\n",
    "test_data_cm_file_name_p90      = '{}_cm_test_p90.png'.format(trained_model_name)\n",
    "test_data_cm_file_name_p95      = '{}_cm_test_p95.png'.format(trained_model_name)\n",
    "\n",
    "model_eval_file_name            = '{}_eval.pkl'.format(trained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = generate_seed()\n",
    "random_state = init_random_generator(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Prepared Data Saved in Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    with open(os.path.join(prepared_data_loc, prepared_data_file_name), 'rb') as file_handle:\n",
    "        prepared_data = pickle.load(file_handle)\n",
    "    print('\\nRead prepared data from \"{}\" at \"{}\"\\n'.format(prepared_data_file_name, prepared_data_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Features and Labels to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    features_in_prep_data = prepared_data['features'].keys()\n",
    "    features_to_use = define_features_to_use (features_in_prep_data, qois_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_in_prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepared_data['labels'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    if (FM_label_type == 'Regression'):\n",
    "        labels_to_use = ['FM_{}hr'.format(FM_hr)]\n",
    "    elif (FM_label_type == 'Binary'):\n",
    "        labels_to_use = ['FM_{}hr_bin'.format(FM_hr)]\n",
    "    elif (FM_label_type == 'MultiClass'):\n",
    "        labels_to_use = ['FM_{}hr_MC'.format(FM_hr)]\n",
    "    else:\n",
    "        raise ValueError('Invalid \"label_type\": {} in \"FM_labels\". \\\n",
    "                        \\nValid types are: \"Regression\", \"MultiClass\", and \"Binary\"'.format(\\\n",
    "                                                                                FM_label_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Labels from Prepared Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    X_tt     = prepared_data['features'][features_to_use]\n",
    "    y_tt     = prepared_data['labels'][labels_to_use]\n",
    "    #idy_tt   = prepared_data['identity']\n",
    "    #all_tt = prepared_data['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tt, y_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tt.dtypes, y_tt.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    print ('Data scaler type: {}'.format(scaler_type))\n",
    "    scaler = define_scaler (scaler_type)\n",
    "    scaler.fit(X_tt)\n",
    "    X_tt_scaled = scaler.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tt_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression' and label_log):\n",
    "    y_tt = np.log(y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(y_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clarify if train/test split should be performed after or before scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train /Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    test_data_frac = json_content_train_model['models']['test_data_frac']\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(\\\n",
    "                             X_tt_scaled, y_tt.to_numpy(), test_size = test_data_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(labels_train), plt.hist(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('FM label type: {}'.format(FM_label_type))\n",
    "print ('ML model considered: {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    model = define_model (FM_label_type, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    print ('The model chosen is: {} \\n'.format(model))\n",
    "    print ('Deafult model params: \\n {}'.format(model.get_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch and model_params != {}:\n",
    "    print ('Updating the model params with the dict: \\n {}'.format(model_params))\n",
    "    model.set_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch and model_params != {}:\n",
    "    print ('Updated model params: \\n {}'.format(model.get_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    t0 = time.time()\n",
    "    model.fit(features_train, labels_train.ravel())\n",
    "    training_time = round(time.time()-t0, 3)\n",
    "    print (\"\\nTraining Time: {} s\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_from_scratch:\n",
    "    trained_model_file = os.path.join(trained_model_loc, trained_model_file_name)\n",
    "    model = pickle.load(open(trained_model_file, 'rb'))\n",
    "    print ('\\nLoaded the ML model file at: {}\\n'.format(trained_model_file))\n",
    "    print ('The model loaded is: {} \\n'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the features and labels used in training if not training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_from_scratch:\n",
    "    print('Loading the saved features and labels used in training')\n",
    "    features_train = pickle.load(open(os.path.join(\\\n",
    "                            trained_model_loc, train_data_features_file_name), 'rb'))\n",
    "    labels_train   =  pickle.load(open(os.path.join(\\\n",
    "                            trained_model_loc, train_data_labels_file_name), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_train = predict(model, features_train, \"Train Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    labels_error, labels_error_abs, labels_pc_err, labels_pc_err_abs = \\\n",
    "                                            compute_errors (labels_train, labels_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    labels_error_p90, labels_error_p95, \\\n",
    "    labels_gt_best90, labels_pred_best90, \\\n",
    "    labels_gt_best95, labels_pred_best95 = \\\n",
    "            compute_best_90_95_labels (labels_train, labels_pred_train, labels_error_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    reg_metrics_train = \\\n",
    "    get_metrics_plot_scatter_regression (labels_train, labels_pred_train, \"Train Data\", \\\n",
    "                             model_name, trained_model_loc, train_data_scatter_file_name, \\\n",
    "                             max_data_size_scatter, fig_size_x, fig_size_y, \\\n",
    "                             font_size, x_lim, label_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Binary' or FM_label_type == 'MultiClass'):\n",
    "    conf_mat_train = get_confusion_matrix (FM_label_type, labels_train, labels_pred_train, \\\n",
    "                                      \"Train Data\", class_labels)\n",
    "    get_classification_report (FM_label_type, labels_train, labels_pred_train, \\\n",
    "                          \"Train Data\", class_labels)\n",
    "    accuracy_train = accuracy_score(labels_train, labels_pred_train)\n",
    "else:\n",
    "    conf_mat_train = None\n",
    "    print('Confusion Matrix is not suitable for label_type: {}'.format(FM_label_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Binary'):\n",
    "    average_precision_train = average_precision_score(labels_train, labels_pred_train)\n",
    "    print('Average precision-recall score for Train Data: {0:0.2f}'.format(\n",
    "          average_precision_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot  Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type != 'Regression'):\n",
    "    plot_confusion_matrix (conf_mat_train, accuracy_train, model_name, \\\n",
    "                           trained_model_loc, train_data_cm_file_name, \\\n",
    "                           fig_size_x, fig_size_y, \\\n",
    "                           font_size,\\\n",
    "                           normalize_cm, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the features and labels saved for testing if not training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_from_scratch:\n",
    "    print('Loading the saved features and labels meant for testing')\n",
    "    features_test = pickle.load(open(os.path.join(\\\n",
    "                            trained_model_loc, test_data_features_file_name), 'rb'))\n",
    "    labels_test   =  pickle.load(open(os.path.join(\\\n",
    "                            trained_model_loc, test_data_labels_file_name), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(labels_error_abs, bins = 50, density=False, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(labels_error_abs, bins = 50, density=True, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_test = predict(model, features_test, \"Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    labels_error, labels_error_abs, labels_pc_err, labels_pc_err_abs = \\\n",
    "                                            compute_errors (labels_test, labels_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    labels_error_p90, labels_error_p95, \\\n",
    "    labels_gt_p90, labels_pred_p90, \\\n",
    "    labels_gt_p95, labels_pred_p95 = \\\n",
    "            compute_best_90_95_labels (labels_test, labels_pred_test, labels_error_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    reg_metrics_test = \\\n",
    "    get_metrics_plot_scatter_regression (labels_test, labels_pred_test, \"Test Data\", \\\n",
    "                             model_name, trained_model_loc, test_data_scatter_file_name, \\\n",
    "                             max_data_size_scatter, fig_size_x, fig_size_y, \\\n",
    "                             font_size, x_lim, label_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    reg_metrics_test_p90 = \\\n",
    "    get_metrics_plot_scatter_regression (labels_gt_p90, labels_pred_p90, \"Test Data -p90\", \\\n",
    "                             model_name, trained_model_loc, test_data_scatter_file_name_p90, \\\n",
    "                             max_data_size_scatter, fig_size_x, fig_size_y, \\\n",
    "                             font_size, x_lim, label_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    reg_metrics_test_p95 = \\\n",
    "    get_metrics_plot_scatter_regression (labels_gt_p95, labels_pred_p95, \"Test Data -p95\", \\\n",
    "                             model_name, trained_model_loc, test_data_scatter_file_name_p95, \\\n",
    "                             max_data_size_scatter, fig_size_x, fig_size_y, \\\n",
    "                             font_size, x_lim, label_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Binary' or FM_label_type == 'MultiClass'):\n",
    "    conf_mat_test = get_confusion_matrix (FM_label_type, labels_test, labels_pred_test, \\\n",
    "                                      \"Test Data\", class_labels)\n",
    "    get_classification_report (FM_label_type, labels_test, labels_pred_test, \\\n",
    "                          \"Test Data\", class_labels)\n",
    "    accuracy_test = accuracy_score(labels_test, labels_pred_test)\n",
    "else:\n",
    "    conf_mat_test = None\n",
    "    print('Confusion Matrix is not suitable for label_type: {}'.format(FM_label_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Binary'):\n",
    "    average_precision_test = average_precision_score(labels_test, labels_pred_test)\n",
    "    print('Average precision-recall score for Test Data: {0:0.2f}'.format(\n",
    "          average_precision_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type != 'Regression'):\n",
    "    plot_confusion_matrix (conf_mat_test, accuracy_test, model_name, \\\n",
    "                           trained_model_loc, test_data_cm_file_name, \\\n",
    "                           fig_size_x, fig_size_y, \\\n",
    "                           font_size,\\\n",
    "                           normalize_cm, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save ML Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type == 'Regression'):\n",
    "    data_for_csv = create_train_test_metrics (reg_metrics_train, reg_metrics_test, \\\n",
    "                                              reg_metrics_test_p90, reg_metrics_test_p95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (FM_label_type != 'Regression'):\n",
    "    data_for_csv = { 'conf_mat_train':    [conf_mat_train],\n",
    "                         'conf_mat_test':     [conf_mat_test]\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_csv = pd.DataFrame(data_for_csv)\n",
    "model_eval_csv.to_csv(os.path.join(trained_model_loc, \\\n",
    "                                   model_eval_file_name.replace('pkl','csv')), \\\n",
    "                                   index=False, float_format = '%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_df = pd.DataFrame(data_for_csv) # Merge two dicts\n",
    "model_eval_df.to_pickle(os.path.join(trained_model_loc, model_eval_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_from_pickle = pd.read_pickle(os.path.join(trained_model_loc, model_eval_file+'.pkl'))\n",
    "#df_from_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch and save_train_data:\n",
    "    pickle.dump(features_train, open(os.path.join(\\\n",
    "                        trained_model_loc, train_data_features_file_name), 'wb'))\n",
    "    pickle.dump(labels_train, open(os.path.join(\\\n",
    "                        trained_model_loc, train_data_labels_file_name), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch and save_test_data:\n",
    "    pickle.dump(features_test, open(os.path.join(\\\n",
    "                        trained_model_loc, test_data_features_file_name), 'wb'))\n",
    "    pickle.dump(labels_test, open(os.path.join(\\\n",
    "                        trained_model_loc, test_data_labels_file_name), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "    trained_model_file = os.path.join(trained_model_loc, trained_model_file_name)\n",
    "    pickle.dump(model, open(trained_model_file, 'wb'))\n",
    "    print ('\\nSaved the ML model file at: {}\\n'.format(trained_model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global End Time and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_final_memory = process.memory_info().rss\n",
    "global_end_time = timer()\n",
    "global_memory_consumed = global_final_memory - global_initial_memory\n",
    "print('Total memory consumed: {:.3f} MB'.format(global_memory_consumed/(1024*1024)))\n",
    "print('Total computing time: {:.3f} s'.format(global_end_time - global_start_time))\n",
    "print('=========================================================================')\n",
    "print(\"SUCCESS: Done Training and Evaluation of ML Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_ml_conda",
   "language": "python",
   "name": "py3_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
