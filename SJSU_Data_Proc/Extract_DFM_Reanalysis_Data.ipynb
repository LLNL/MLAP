{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/g92/jha3/VirtualEnv/py3_ml_wind/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as path\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pickle\n",
    "#from matplotlib import pyplot as plt\n",
    "#plt.style.use('seaborn-white')\n",
    "from datetime import date, datetime, timedelta, time\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_extract_wrf import generate_seed, init_random_generator\n",
    "from helper_extract_wrf import get_data_file_names, downsample_data_files\n",
    "from helper_extract_wrf import downsample_grid_indices\n",
    "from helper_extract_wrf import create_df_at_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to be used for extracting WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nidentity_fields = ['latitude', 'longitude', 'YYYY', 'MM', 'DD', 'HH']\\n\\nlabel_fields = ['mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',                'mean_wtd_moisture_100hr', 'mean_wtd_moisture_1000hr']\\n\\nfeature_fields = ['eastward_10m_wind', 'northward_10m_wind',                  'air_temperature_2m',                   'accumulated_precipitation_amount',                   'air_relative_humidity_2m',                   'surface_downwelling_shortwave_flux'] \\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRF data set location and the extracted data set location\n",
    "data_files_location = '/p/vast1/climres/DFM_reanalysis'\n",
    "extracted_data_loc = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/SJSU/01_WRF_Nelson_Data_Extracted'\n",
    "\n",
    "# The current data set params\n",
    "data_set_count = 0\n",
    "percent_files_to_use = 0.02         # f1 = what percent of available files to use\n",
    "percent_grid_points_to_use = 0.005  # f2 = what percent of grid points to use\n",
    "max_history_to_consider = 5 # n_history in hours\n",
    "history_interval        = 2\n",
    "\n",
    "# Some fixed stuff\n",
    "'''\n",
    "identity_fields = ['latitude', 'longitude', 'YYYY', 'MM', 'DD', 'HH']\n",
    "\n",
    "label_fields = ['mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',\\\n",
    "                'mean_wtd_moisture_100hr', 'mean_wtd_moisture_1000hr']\n",
    "\n",
    "feature_fields = ['eastward_10m_wind', 'northward_10m_wind',\\\n",
    "                  'air_temperature_2m', \\\n",
    "                  'accumulated_precipitation_amount', \\\n",
    "                  'air_relative_humidity_2m', \\\n",
    "                  'surface_downwelling_shortwave_flux'] \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = generate_seed()\n",
    "random_state = init_random_generator(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting the names of the data files at the dir : \n",
      " /p/vast1/climres/DFM_reanalysis \n",
      "\n",
      "years_list: ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'] \n",
      "\n",
      "Getting the names of the data files for the year : 2000\n",
      "... Found 8784 files for this year\n",
      "Getting the names of the data files for the year : 2001\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2002\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2003\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2004\n",
      "... Found 8784 files for this year\n",
      "Getting the names of the data files for the year : 2005\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2006\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2007\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2008\n",
      "... Found 8784 files for this year\n",
      "Getting the names of the data files for the year : 2009\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2010\n",
      "... Found 8759 files for this year\n",
      "Getting the names of the data files for the year : 2011\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2012\n",
      "... Found 8784 files for this year\n",
      "Getting the names of the data files for the year : 2013\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2014\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2015\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2016\n",
      "... Found 8784 files for this year\n",
      "Getting the names of the data files for the year : 2017\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2018\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2019\n",
      "... Found 8760 files for this year\n",
      "Getting the names of the data files for the year : 2020\n",
      "... Found 8784 files for this year\n",
      "\n",
      "Found a total of 184103 files \n",
      "\n",
      "=========================================================================\n",
      "Module \"get_data_file_names\" computing time: 0.2973566269502044 s\n"
     ]
    }
   ],
   "source": [
    "module_start_time = timer()\n",
    "data_files_list = get_data_file_names(data_files_location)\n",
    "module_end_time = timer()\n",
    "print('Module \"get_data_file_names\" computing time: {} s'.format(module_end_time - module_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Randomly selecting approx 0.02 % of the data files\n",
      "Selected 37 data files out of 184103\n",
      "Indices of the randomly selected files: \n",
      " [112379, 131207, 134488, 63266, 144535, 154953, 102091, 62994, 99309, 129072, 114855, 27362, 43842, 173591, 6255, 136922, 84356, 11649, 84688, 294, 146031, 61194, 41708, 85863, 150674, 58399, 131677, 182062, 77552, 22259, 94242, 132352, 166030, 137872, 173209, 59883, 141851]\n",
      "Names of the randomly selected files: \n",
      " ['wrf_2012-10-26_12.nc', 'wrf_2014-12-20_00.nc', 'wrf_2015-05-05_17.nc', 'wrf_2007-03-21_02.nc', 'wrf_2016-06-27_08.nc', 'wrf_2017-09-04_10.nc', 'wrf_2011-08-24_20.nc', 'wrf_2007-03-09_18.nc', 'wrf_2011-04-30_22.nc', 'wrf_2014-09-22_01.nc', 'wrf_2013-02-06_16.nc', 'wrf_2003-02-14_02.nc', 'wrf_2004-12-31_18.nc', 'wrf_2019-10-21_00.nc', 'wrf_2000-09-17_15.nc', 'wrf_2015-08-15_03.nc', 'wrf_2009-08-15_20.nc', 'wrf_2001-04-30_09.nc', 'wrf_2009-08-29_16.nc', 'wrf_2000-01-13_06.nc', 'wrf_2016-08-28_16.nc', 'wrf_2006-12-24_18.nc', 'wrf_2004-10-03_20.nc', 'wrf_2009-10-17_15.nc', 'wrf_2017-03-10_03.nc', 'wrf_2006-08-30_07.nc', 'wrf_2015-01-08_14.nc', 'wrf_2020-10-07_23.nc', 'wrf_2008-11-05_08.nc', 'wrf_2002-07-16_11.nc', 'wrf_2010-10-01_19.nc', 'wrf_2015-02-05_17.nc', 'wrf_2018-12-09_23.nc', 'wrf_2015-09-23_17.nc', 'wrf_2019-10-05_02.nc', 'wrf_2006-10-31_03.nc', 'wrf_2016-03-07_12.nc']\n",
      "=========================================================================\n",
      "Module \"downsample_data_files\" computing time: 0.03559145284816623 s\n"
     ]
    }
   ],
   "source": [
    "module_start_time = timer()\n",
    "sampled_file_indices, sampled_data_files = downsample_data_files (data_files_list, percent_files_to_use)\n",
    "module_end_time = timer()\n",
    "print('Module \"downsample_data_files\" computing time: {} s'.format(module_end_time - module_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global End Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total computing time: 0.3706265729852021 s\n"
     ]
    }
   ],
   "source": [
    "global_end_time = timer()\n",
    "print('Total computing time: {} s'.format(global_end_time - global_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells below this are old/legacy stuff for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Dimensions, Downsample Grid Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_for_all_files = pd.DataFrame()\n",
    "for file_count, data_file_name in enumerate(sampled_data_files):\n",
    "    print ('\\nReading data from file # {}, with name :- {}'.format(file_count, data_file_name))\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    dfm_file_data = xr.open_dataset(path.join(data_files_location, data_file_name))\n",
    "    \n",
    "    df_for_single_file = downsample_grid_indices (data_file_name,dfm_file_data, percent_grid_points_to_use, \n",
    "                                                  max_history_to_consider, history_interval, frames_in_file)\n",
    "    \n",
    "    df_for_all_files = df_for_all_files.append(df_for_single_file).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_for_all_files.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_name = 'extracted_data_%02d'%(data_set_count)\n",
    "extracted_data_file_name = '{}.pkl'.format(data_set_name)\n",
    "'''\n",
    "extracted_data_file_name = '{}_files_{}pc_grid_points_{}pc_max_history_{}_hist_interval_{}.pkl'.format(\n",
    "                            data_set_name, # name of data set\n",
    "                            percent_files_to_use, # f1 = what percent of available files to use\n",
    "                            percent_grid_points_to_use, # f2 = what percent of grid points to use\n",
    "                            max_history_to_consider, # n_history in hours\n",
    "                            history_interval)\n",
    "'''\n",
    "extracted_data = {'percent_files_to_use': [percent_files_to_use],\n",
    "                 'percent_grid_points_to_use': [percent_grid_points_to_use],\n",
    "                 'max_history_to_consider': [max_history_to_consider],\n",
    "                 'history_interval': [history_interval],\n",
    "                 'number_of_files_used' : [len(sampled_data_files)],\n",
    "                 'number_of_data_points' : [len(df_for_all_files)],\n",
    "                 'df_for_all_files': df_for_all_files}\n",
    "extracted_data_file_handle = open(os.path.join(\n",
    "    extracted_data_loc, extracted_data_file_name), 'wb')\n",
    "pickle.dump(extracted_data, extracted_data_file_handle)\n",
    "extracted_data_file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del extracted_data['df_for_all_files']\n",
    "#extracted_data['index'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_file_name = '{}.csv'.format(os.path.join(extracted_data_loc, data_set_name))\n",
    "tabulated_data = pd.DataFrame.from_dict(extracted_data).reset_index(drop = True)\n",
    "tabulated_data.to_csv(tab_data_file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabulated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load extracted data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(os.path.join(\n",
    "    extracted_data_loc, extracted_data_file_name), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_data['df_for_all_files'][5:15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Wind",
   "language": "python",
   "name": "py3_ml_wind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
