{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/g92/jha3/VirtualEnv/py3_ml_wind/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pickle\n",
    "#from matplotlib import pyplot as plt\n",
    "#plt.style.use('seaborn-white')\n",
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_extract_wrf import generate_seed, init_random_generator\n",
    "from helper_extract_wrf import get_data_file_names, downsample_data_files\n",
    "from helper_extract_wrf import downsample_grid_indices\n",
    "from helper_extract_wrf import create_df_at_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to be used for extracting WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRF data set location and the extracted data set location\n",
    "data_files_location = '/p/lustre1/mirocha2/SJSU_DATA/akochanski/PGnE_climo/dfm'\n",
    "extracted_data_loc = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/SJSU/01_WRF_Nelson_Data_Extracted'\n",
    "\n",
    "# The current data set params\n",
    "data_set_count = 0\n",
    "percent_files_to_use = 2.0         # f1 = what percent of available files to use\n",
    "percent_grid_points_to_use = 0.005  # f2 = what percent of grid points to use\n",
    "max_history_to_consider = 5 # n_history in hours\n",
    "history_interval        = 2\n",
    "\n",
    "# Some fixed stuff\n",
    "frames_in_file          = 153\n",
    "\n",
    "identity_fields = ['latitude', 'longitude', 'YYYY', 'MM', 'DD', 'HH']\n",
    "\n",
    "label_fields = ['mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',\\\n",
    "                'mean_wtd_moisture_100hr', 'mean_wtd_moisture_1000hr']\n",
    "\n",
    "feature_fields = ['eastward_10m_wind', 'northward_10m_wind',\\\n",
    "                  'air_temperature_2m', \\\n",
    "                  'accumulated_precipitation_amount', \\\n",
    "                  'air_relative_humidity_2m', \\\n",
    "                  'surface_downwelling_shortwave_flux'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = generate_seed()\n",
    "random_state = init_random_generator(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths, File Names, Downsample Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting the names of data files at the dir : \n",
      " /p/lustre1/mirocha2/SJSU_DATA/akochanski/PGnE_climo/dfm\n",
      "Found 99 files\n",
      "=========================================================================\n",
      "\n",
      "Randomly selecting approx 2.0 % of the data files\n",
      "Selected 2 data files\n",
      "Indices of the randomly selected files: \n",
      " [20, 85]\n",
      "Names of the randomly selected files: \n",
      " ['wrfout_d03_1989-12-05_00:00:00_dfm.nc', 'wrfout_d03_1990-10-26_00:00:00_dfm.nc']\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "data_files_list = get_data_file_names(data_files_location)\n",
    "sampled_file_indices, sampled_data_files = downsample_data_files (data_files_list, percent_files_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Dimensions, Downsample Grid Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data from file # 0, with name :- wrfout_d03_1989-12-05_00:00:00_dfm.nc\n",
      "-----------------------------------------------------------------------\n",
      "Selecting 10 grid points (approx 0.005 % of a total of 190080 grid points)\n",
      "\n",
      "\n",
      "Reading data from file # 1, with name :- wrfout_d03_1990-10-26_00:00:00_dfm.nc\n",
      "-----------------------------------------------------------------------\n",
      "Selecting 10 grid points (approx 0.005 % of a total of 190080 grid points)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_for_all_files = pd.DataFrame()\n",
    "for file_count, data_file_name in enumerate(sampled_data_files):\n",
    "    print ('\\nReading data from file # {}, with name :- {}'.format(file_count, data_file_name))\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    dfm_file_data = xr.open_dataset(path.join(data_files_location, data_file_name))\n",
    "    \n",
    "    df_for_single_file = downsample_grid_indices (data_file_name,dfm_file_data, percent_grid_points_to_use, \n",
    "                                                  max_history_to_consider, history_interval, frames_in_file)\n",
    "    \n",
    "    df_for_all_files = df_for_all_files.append(df_for_single_file).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_for_all_files.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_name = 'extracted_data_%02d'%(data_set_count)\n",
    "extracted_data_file_name = '{}.pkl'.format(data_set_name)\n",
    "'''\n",
    "extracted_data_file_name = '{}_files_{}pc_grid_points_{}pc_max_history_{}_hist_interval_{}.pkl'.format(\n",
    "                            data_set_name, # name of data set\n",
    "                            percent_files_to_use, # f1 = what percent of available files to use\n",
    "                            percent_grid_points_to_use, # f2 = what percent of grid points to use\n",
    "                            max_history_to_consider, # n_history in hours\n",
    "                            history_interval)\n",
    "'''\n",
    "extracted_data = {'percent_files_to_use': [percent_files_to_use],\n",
    "                 'percent_grid_points_to_use': [percent_grid_points_to_use],\n",
    "                 'max_history_to_consider': [max_history_to_consider],\n",
    "                 'history_interval': [history_interval],\n",
    "                 'number_of_files_used' : [len(sampled_data_files)],\n",
    "                 'number_of_data_points' : [len(df_for_all_files)],\n",
    "                 'df_for_all_files': df_for_all_files}\n",
    "extracted_data_file_handle = open(os.path.join(\n",
    "    extracted_data_loc, extracted_data_file_name), 'wb')\n",
    "pickle.dump(extracted_data, extracted_data_file_handle)\n",
    "extracted_data_file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del extracted_data['df_for_all_files']\n",
    "#extracted_data['index'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_file_name = '{}.csv'.format(os.path.join(extracted_data_loc, data_set_name))\n",
    "tabulated_data = pd.DataFrame.from_dict(extracted_data).reset_index(drop = True)\n",
    "tabulated_data.to_csv(tab_data_file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabulated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load extracted data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(os.path.join(\n",
    "    extracted_data_loc, extracted_data_file_name), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_data['df_for_all_files'][5:15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Wind",
   "language": "python",
   "name": "py3_ml_wind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
