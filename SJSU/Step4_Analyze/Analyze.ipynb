{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert this notebook to executable python script using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jupyter nbconvert --to python Analyze.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as path\n",
    "import psutil\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pickle\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from datetime import date, datetime, timedelta, time\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "#from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_running_file_dir = sys.path[0]\n",
    "current_running_file_par = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, os.path.join(current_running_file_par, 'Step1_ExtractData'))\n",
    "sys.path.insert(0, os.path.join(current_running_file_par, 'Step2_PrepareData'))\n",
    "sys.path.insert(0, os.path.join(current_running_file_par, 'Step3_TrainModel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extract_DFM_Data_Helper import *\n",
    "from Prepare_TrainTest_Data_Helper import *\n",
    "from TrainModel_Helper import *\n",
    "from Analyze_Helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Start Time and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = timer()\n",
    "process = psutil.Process(os.getpid())\n",
    "global_initial_memory = process.memory_info().rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Input JSON File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file name when using jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_extract_data = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/01_WRF_Nelson_Data_Extracted/InputJsonFiles/json_extract_data_005.json'\n",
    "json_file_prep_data    = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/02_TrainTest_Data_Prepared/InputJsonFiles/json_prep_data_label_000.json'\n",
    "json_file_train_model  = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/03_Trained_Models/InputJsonFiles/json_train_model_002.json'\n",
    "json_file_analyze      = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/04_Analysis/InputJsonFiles/json_analyze_000.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file name when using python script on command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_file_extract_data = sys.argv[1]\n",
    "#json_file_prep_data = sys.argv[2]\n",
    "#json_file_train_model = sys.argv[3]\n",
    "#json_file_analyze  = sys.argv[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for extracting data: \\n {}'.format(json_file_extract_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_extract_data) as json_file_handle:\n",
    "    json_content_extract_data = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_extract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for preparing data: \\n {}'.format(json_file_prep_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_prep_data) as json_file_handle:\n",
    "    json_content_prep_data = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_prep_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for training model: \\n {}'.format(json_file_train_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_train_model) as json_file_handle:\n",
    "    json_content_train_model = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the JSON file for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the JSON file for analysis: \\n {}'.format(json_file_analyze))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_analyze) as json_file_handle:\n",
    "    json_content_analyze = json.load(json_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_content_analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to be Used for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current data set params\n",
    "data_set_count = json_content_extract_data['data_set_defn']['data_set_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Label, FM Threshold etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = json_content_prep_data['label_defn']['label_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_labels = json_content_prep_data['FM_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_label_type = FM_labels['label_type']\n",
    "\n",
    "if (FM_label_type == 'Binary'):\n",
    "    FM_binary_threshold = FM_labels['FM_binary_threshold']\n",
    "if (FM_label_type == 'MultiClass'):\n",
    "    FM_MC_levels = FM_labels['FM_MC_levels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_hr = json_content_prep_data['qoi_to_plot']['FM_hr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML Model and Params etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count = json_content_train_model['models']['model_count']\n",
    "scaler_type = json_content_train_model['models']['scaler_type']\n",
    "model_name = json_content_train_model['models']['model_name'] # ['RF', SVM', 'MLP']\n",
    "model_params = json_content_train_model['models']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Analysis Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data_paths = json_content_analyze['paths']\n",
    "analysis_data_desired = json_content_analyze['analysis_data_desired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data_defined = [analysis_data_elem \\\n",
    "                         for analysis_data_elem in analysis_data_desired \\\n",
    "                         if analysis_data_elem in json_content_analyze]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Analysis desired to be performed on the following data sets:\\n {}'.format(\\\n",
    "                                                            analysis_data_desired))\n",
    "\n",
    "print ('Time and Region Info available for these data sets out of those desired:\\n {}'\\\n",
    "                                                    .format(analysis_data_defined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_region_info = get_time_region_info (analysis_data_defined, json_content_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_base_loc = json_content_train_model['paths']['trained_model_base_loc']\n",
    "analysis_data_base_loc = json_content_analyze['paths']['analysis_data_base_loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_paths = json_content_analyze['paths']['raw_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet, Label, and Model Specific (Trained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = 'dataset_%03d_label_%03d_%s_model_%03d_%s'%(data_set_count, \\\n",
    "                                                        label_count, FM_label_type, \\\n",
    "                                                        model_count, model_name)\n",
    "\n",
    "trained_model_loc = os.path.join(trained_model_base_loc, trained_model_name)\n",
    "os.system('mkdir -p %s'%trained_model_loc)\n",
    "\n",
    "trained_model_file_name = '{}.pkl'.format(trained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for Fire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_fire)\n",
    "X_fire_scaled = scaler.transform(X_fire)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('FM label type: {}'.format(FM_label_type))\n",
    "print ('ML model considered: {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_file = os.path.join(trained_model_loc, trained_model_file_name)\n",
    "model = pickle.load(open(trained_model_file, 'rb'))\n",
    "print ('\\nLoaded the ML model file at: {}\\n'.format(trained_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The model loaded is: {} \\n'.format(model))\n",
    "print ('Model params: \\n {}'.format(model.get_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "labels_pred = model.predict(features_train)\n",
    "print (\"Prediction Time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "    accuracy = accuracy_score(labels_pred, labels_train)\n",
    "else:\n",
    "    accuracy = model.score(features_train, labels_train)\n",
    "conf_mat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin'):\n",
    "    conf_mat = confusion_matrix(labels_train, labels_pred, labels = [0, 1])\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(labels_train, labels_pred, labels=[0, 1]))\n",
    "    average_precision = average_precision_score(labels_train, labels_pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "          average_precision))\n",
    "elif (label_type == 'MC'):\n",
    "    conf_mat = confusion_matrix(labels_train, labels_pred, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "else:\n",
    "    print('Confusion Mat is not suitable for label_type: {}'.format(label_type))\n",
    "\n",
    "print('Accuracy Score: {}'.format(accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "labels_pred = model.predict(features_test)\n",
    "print (\"Prediction Time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "    accuracy = accuracy_score(labels_pred, labels_test)\n",
    "else:\n",
    "    accuracy = model.score(features_test, labels_test)\n",
    "conf_mat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin'):\n",
    "    conf_mat = confusion_matrix(labels_test, labels_pred, labels = [0, 1])\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(labels_test, labels_pred, labels=[0, 1]))\n",
    "    average_precision = average_precision_score(labels_test, labels_pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "          average_precision))\n",
    "elif (label_type == 'MC'):\n",
    "    conf_mat = confusion_matrix(labels_test, labels_pred, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "else:\n",
    "    print('Confusion Mat is not suitable for label_type: {}'.format(label_type))\n",
    "\n",
    "print('Accuracy Score: {}'.format(accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Fire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "labels_pred = model.predict(X_fire_scaled)\n",
    "print (\"Prediction Time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "    accuracy = accuracy_score(labels_pred, y_fire)\n",
    "else:\n",
    "    accuracy = model.score(X_fire_scaled, y_fire)\n",
    "conf_mat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin'):\n",
    "    conf_mat = confusion_matrix(y_fire, labels_pred, labels = [0, 1])\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(y_fire, labels_pred, labels=[0, 1]))\n",
    "    average_precision = average_precision_score(y_fire, labels_pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "          average_precision))\n",
    "elif (label_type == 'MC'):\n",
    "    conf_mat = confusion_matrix(y_fire, labels_pred, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "else:\n",
    "    print('Confusion Mat is not suitable for label_type: {}'.format(label_type))\n",
    "\n",
    "print('Accuracy Score: {}'.format(accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Ground Truth and Prediction of Fire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny, nx = (480, 396)\n",
    "j_indices = idy_fire['j_ind']\n",
    "i_indices = idy_fire['i_ind']\n",
    "ground_truth = y_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_type == 'Regr':\n",
    "    ground_truth_mat = np.full((ny,nx), np.nan)\n",
    "    pred_mat = np.full_like(ground_truth_mat, np.nan )\n",
    "    error_mat = np.full_like(ground_truth_mat, np.nan)\n",
    "else:\n",
    "    ground_truth_mat = np.ones((ny,nx), int)*(-1)\n",
    "    pred_mat = np.ones_like(ground_truth_mat, int)*(-1)\n",
    "    error_mat = np.ones_like(ground_truth_mat, int)*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j_loc, i_loc, gt_val, pred_val in zip (j_indices, i_indices, ground_truth, labels_pred):\n",
    "    ground_truth_mat[j_loc][i_loc] = gt_val\n",
    "    pred_mat[        j_loc][i_loc] = pred_val\n",
    "    if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "        error_mat[       j_loc][i_loc] = (gt_val == pred_val)\n",
    "    else:\n",
    "        error_mat[       j_loc][i_loc] = 100.0*(pred_val/gt_val - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_name = input_json_data['plot_options']['cmap_name']\n",
    "cont_levels = input_json_data['plot_options']['cont_levels']\n",
    "cont_levels = np.linspace(0, 0.28, 21)\n",
    "cont_levels_err = np.linspace(-75.0, 75.0, 21)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "x_ind, y_ind = np.meshgrid(range(nx), range(ny))\n",
    "\n",
    "cont = ax[0].contourf(x_ind, y_ind, ground_truth_mat, levels = cont_levels, cmap=cmap_name, extend='both')\n",
    "plt.colorbar(cont)\n",
    "ax[0].set_title('Ground Truth')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "cont = ax[1].contourf(x_ind, y_ind, pred_mat, levels = cont_levels, cmap=cmap_name, extend='both')\n",
    "plt.colorbar(cont)\n",
    "ax[1].set_title('Prediction')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "cont = ax[2].contourf(x_ind, y_ind, error_mat, levels = cont_levels_err, cmap=cmap_name, extend='both')\n",
    "plt.colorbar(cont)\n",
    "ax[2].set_title('Correct Match')\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "\n",
    "filename = trained_model_name.split('.')[0] + '_{}_Fire.png'.format(fire_name)\n",
    "filedir = analysis_loc\n",
    "os.system('mkdir -p %s'%filedir)\n",
    "\n",
    "plt.savefig(os.path.join(filedir, filename), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global End Time and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_final_memory = process.memory_info().rss\n",
    "global_end_time = timer()\n",
    "global_memory_consumed = global_final_memory - global_initial_memory\n",
    "print('Total memory consumed: {:.3f} MB'.format(global_memory_consumed/(1024*1024)))\n",
    "print('Total computing time: {:.3f} s'.format(global_end_time - global_start_time))\n",
    "print('=========================================================================')\n",
    "print(\"SUCCESS: Done Training and Testing of Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_ml_conda",
   "language": "python",
   "name": "py3_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
