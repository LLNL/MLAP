{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert this notebook to executable python script using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as path\n",
    "import psutil\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from datetime import date, datetime, timedelta, time\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(os.path.pardir, 'Step1_ExtractData'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extract_DFM_Data_Helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Start Time and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = timer()\n",
    "process = psutil.Process(os.getpid())\n",
    "global_initial_memory = process.memory_info().rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to be Used for Preparing Train, Test, and Fire Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current data set params\n",
    "data_set_count = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model, Label etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = 'Regr' # ['bin', 'MC', 'Regr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_hr = 10 # [10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_considered = 'MLP' # ['RF', SVM', 'MLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_name = 'Woosley'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_base_loc = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/ArchivedData/data_archived_2023_09_21/02_TrainTestFire_Data_Prepared'\n",
    "trained_model_base_loc = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/03_Trained_Models'\n",
    "analysis_data_base_loc = '/p/lustre2/jha3/Wildfire/Wildfire_LDRD_SI/04_Analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet Specific (Train, Test, Fire Prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_set_name = 'data_prepared_%02d'%(data_set_count)\n",
    "\n",
    "prepared_data_loc = os.path.join(prepared_data_base_loc, prepared_data_set_name)\n",
    "os.system('mkdir -p %s'%prepared_data_loc)\n",
    "\n",
    "prepared_data_file_name = '{}.pkl'.format(prepared_data_set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet Specific (Trained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_set_name = 'trained_model_%02d'%(data_set_count)\n",
    "\n",
    "trained_model_loc = os.path.join(trained_model_base_loc, trained_model_set_name)\n",
    "os.system('mkdir -p %s'%trained_model_loc)\n",
    "\n",
    "trained_model_name = '{}_{}_{}.pkl'.format(trained_model_set_name, label_type, model_considered)\n",
    "trained_model_file = os.path.join(trained_model_loc, trained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet Specific (Analysis Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_set_name = 'analysis_%02d'%(data_set_count)\n",
    "\n",
    "analysis_loc = os.path.join(analysis_data_base_loc, analysis_set_name, label_type, model_considered)\n",
    "os.system('mkdir -p %s'%analysis_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = generate_seed()\n",
    "random_state = init_random_generator(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The The Prepared Data Saved in Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_file_handle = open(os.path.join(prepared_data_loc, prepared_data_file_name), 'rb')\n",
    "prepared_data = pickle.load(prepared_data_file_handle)\n",
    "prepared_data_file_handle.close()\n",
    "print('Read prepared data from \"{}\" at \"{}\"'.format(prepared_data_file_name, prepared_data_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Features and Labels to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data['tt']['all']\n",
    "#prepared_data['fire']['Woosley']['identity'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Headers for Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = prepared_data['tt']['features'].keys()\n",
    "if (label_type == 'Regr'):\n",
    "    label_to_use = 'FM_{}hr'.format(FM_hr)\n",
    "else:\n",
    "    label_to_use = 'FM_{}hr_{}'.format(FM_hr, label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tt     = prepared_data['tt']['features'][features_to_use]\n",
    "y_tt     = prepared_data['tt']['labels'][label_to_use]\n",
    "idy_tt   = prepared_data['tt']['identity']\n",
    "\n",
    "X_fire   = prepared_data['fire'][fire_name]['features'][features_to_use]\n",
    "y_fire   = prepared_data['fire'][fire_name]['labels'][label_to_use]\n",
    "idy_fire = prepared_data['fire'][fire_name]['identity']\n",
    "\n",
    "#all = prepared_data['tt']['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_tt)\n",
    "X_tt_scaled = scaler.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tt_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for Fire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_fire)\n",
    "X_fire_scaled = scaler.transform(X_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_fire_scaled.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train /Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(X_tt_scaled, y_tt, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('label_type: {}'.format(label_type))\n",
    "print ('Model: {}'.format(model_considered))\n",
    "\n",
    "if (label_type == 'Regr'):\n",
    "    match model_considered:\n",
    "        case 'SVM':\n",
    "            model = SVR(kernel='rbf')\n",
    "        case 'RF':\n",
    "            model = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "        case 'MLP':\n",
    "            model = MLPRegressor(random_state=1, max_iter=500)\n",
    "else: # 'bin' or 'MC'\n",
    "    match model_considered:\n",
    "        case 'SVM':\n",
    "            model = SVC(kernel=\"linear\", class_weight = \"balanced\")\n",
    "        case 'RF':\n",
    "            model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "        case 'MLP':\n",
    "            model = MLPClassifier(solver = 'sgd', activation = 'relu', max_iter= 10000, \n",
    "                    random_state = 0, hidden_layer_sizes = [15,15,15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "model.fit(features_train, labels_train)\n",
    "print (\"Training Time:\", round(time.time()-t0, 3), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(trained_model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(trained_model_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "labels_pred = model.predict(features_train)\n",
    "print (\"Prediction Time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "    accuracy = accuracy_score(labels_pred, labels_train)\n",
    "else:\n",
    "    accuracy = model.score(features_train, labels_train)\n",
    "conf_mat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin'):\n",
    "    conf_mat = confusion_matrix(labels_train, labels_pred, labels = [0, 1])\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(labels_train, labels_pred, labels=[0, 1]))\n",
    "    average_precision = average_precision_score(labels_train, labels_pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "          average_precision))\n",
    "elif (label_type == 'MC'):\n",
    "    conf_mat = confusion_matrix(labels_train, labels_pred, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "else:\n",
    "    print('Confusion Mat is not suitable for label_type: {}'.format(label_type))\n",
    "\n",
    "print('Accuracy Score: {}'.format(accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "labels_pred = model.predict(features_test)\n",
    "print (\"Prediction Time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "    accuracy = accuracy_score(labels_pred, labels_test)\n",
    "else:\n",
    "    accuracy = model.score(features_test, labels_test)\n",
    "conf_mat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin'):\n",
    "    conf_mat = confusion_matrix(labels_test, labels_pred, labels = [0, 1])\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(labels_test, labels_pred, labels=[0, 1]))\n",
    "    average_precision = average_precision_score(labels_test, labels_pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "          average_precision))\n",
    "elif (label_type == 'MC'):\n",
    "    conf_mat = confusion_matrix(labels_test, labels_pred, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "else:\n",
    "    print('Confusion Mat is not suitable for label_type: {}'.format(label_type))\n",
    "\n",
    "print('Accuracy Score: {}'.format(accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Fire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "labels_pred = model.predict(X_fire_scaled)\n",
    "print (\"Prediction Time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "    accuracy = accuracy_score(labels_pred, y_fire)\n",
    "else:\n",
    "    accuracy = model.score(X_fire_scaled, y_fire)\n",
    "conf_mat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (label_type == 'bin'):\n",
    "    conf_mat = confusion_matrix(y_fire, labels_pred, labels = [0, 1])\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(y_fire, labels_pred, labels=[0, 1]))\n",
    "    average_precision = average_precision_score(y_fire, labels_pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "          average_precision))\n",
    "elif (label_type == 'MC'):\n",
    "    conf_mat = confusion_matrix(y_fire, labels_pred, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "else:\n",
    "    print('Confusion Mat is not suitable for label_type: {}'.format(label_type))\n",
    "\n",
    "print('Accuracy Score: {}'.format(accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Ground Truth and Prediction of Fire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny, nx = (480, 396)\n",
    "j_indices = idy_fire['j_ind']\n",
    "i_indices = idy_fire['i_ind']\n",
    "ground_truth = y_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_type == 'Regr':\n",
    "    ground_truth_mat = np.full((ny,nx), np.nan)\n",
    "    pred_mat = np.full_like(ground_truth_mat, np.nan )\n",
    "    error_mat = np.full_like(ground_truth_mat, np.nan)\n",
    "else:\n",
    "    ground_truth_mat = np.ones((ny,nx), int)*(-1)\n",
    "    pred_mat = np.ones_like(ground_truth_mat, int)*(-1)\n",
    "    error_mat = np.ones_like(ground_truth_mat, int)*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j_loc, i_loc, gt_val, pred_val in zip (j_indices, i_indices, ground_truth, labels_pred):\n",
    "    ground_truth_mat[j_loc][i_loc] = gt_val\n",
    "    pred_mat[        j_loc][i_loc] = pred_val\n",
    "    if (label_type == 'bin' or 'label_type' == 'MC'):\n",
    "        error_mat[       j_loc][i_loc] = (gt_val == pred_val)\n",
    "    else:\n",
    "        error_mat[       j_loc][i_loc] = 100.0*(pred_val/gt_val - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_name = 'hot'\n",
    "cont_levels = [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "cont_levels = np.linspace(0, 0.28, 21)\n",
    "cont_levels_err = np.linspace(-75.0, 75.0, 21)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "x_ind, y_ind = np.meshgrid(range(nx), range(ny))\n",
    "\n",
    "cont = ax[0].contourf(x_ind, y_ind, ground_truth_mat, levels = cont_levels, cmap=cmap_name, extend='both')\n",
    "plt.colorbar(cont)\n",
    "ax[0].set_title('Ground Truth')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "cont = ax[1].contourf(x_ind, y_ind, pred_mat, levels = cont_levels, cmap=cmap_name, extend='both')\n",
    "plt.colorbar(cont)\n",
    "ax[1].set_title('Prediction')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "cont = ax[2].contourf(x_ind, y_ind, error_mat, levels = cont_levels_err, cmap=cmap_name, extend='both')\n",
    "plt.colorbar(cont)\n",
    "ax[2].set_title('Correct Match')\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "\n",
    "filename = trained_model_name.split('.')[0] + '_{}_Fire.png'.format(fire_name)\n",
    "filedir = analysis_loc\n",
    "os.system('mkdir -p %s'%filedir)\n",
    "\n",
    "plt.savefig(os.path.join(filedir, filename), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global End Time and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_final_memory = process.memory_info().rss\n",
    "global_end_time = timer()\n",
    "global_memory_consumed = global_final_memory - global_initial_memory\n",
    "print('Total memory consumed: {:.3f} MB'.format(global_memory_consumed/(1024*1024)))\n",
    "print('Total computing time: {:.3f} s'.format(global_end_time - global_start_time))\n",
    "print('=========================================================================')\n",
    "print(\"SUCCESS: Done Training and Testing of Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_ml_conda",
   "language": "python",
   "name": "py3_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
